========================================== Start Test ==========================================

=> loading checkpoint './results/SGD_Cosine_Lr0.01_DN4_ResNet12_Epoch_30_miniImageNet_84_84_5Way_1Shot/model_best.pth.tar'
=> loaded checkpoint './results/SGD_Cosine_Lr0.01_DN4_ResNet12_Epoch_30_miniImageNet_84_84_5Way_1Shot/model_best.pth.tar' (epoch 28)
Namespace(adam=False, aug_shot_num=20, beta1=0.5, clamp_lower=-0.01, clamp_upper=0.01, classifier_model='DN4', cosine=True, cuda=True, current_epoch=29, data_name='miniImageNet', dataset_dir='/data1/Liwenbin/Datasets/miniImageNet--ravi', encoder_model='ResNet12', episodeSize=1, episode_test_num=1000, episode_train_num=10000, episode_val_num=1000, epochs=30, imageSize=84, lr=0.01, lr_decay_epochs=[60, 80], lr_decay_rate=0.1, mode='train', momentum=0.9, nc=3, neighbor_k=3, ngpu=1, outf='./results/SGD_Cosine_Lr0.01_DN4_ResNet12_Epoch_30_miniImageNet_84_84_5Way_1Shot', print_freq=100, query_num=15, resume='', shot_num=1, start_epoch=0, test_aug=False, testepisodeSize=1, train_aug=True, way_num=5, weight_decay=0.0005, workers=4)
Fewshot_model(
  (features): ResNet_84(
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.1)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (downsample): Sequential(
          (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (DropBlock): DropBlock()
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.1)
        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (downsample): Sequential(
          (0): Conv2d(64, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (DropBlock): DropBlock()
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.1)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (downsample): Sequential(
          (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (DropBlock): DropBlock()
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): LeakyReLU(negative_slope=0.1)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn3): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (downsample): Sequential(
          (0): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (DropBlock): DropBlock()
      )
    )
  )
  (classifier): ImgtoClass_Metric()
)
==================== The 0-th round ====================
Val-(28): [100/1000]	Time 0.05 (0.10)	Loss 1.22 (1.05)	Prec@1 54.67 (61.43)
Val-(28): [200/1000]	Time 0.05 (0.09)	Loss 0.88 (1.07)	Prec@1 66.67 (60.48)
Val-(28): [300/1000]	Time 0.13 (0.09)	Loss 1.69 (1.06)	Prec@1 46.67 (60.99)
Val-(28): [400/1000]	Time 0.05 (0.09)	Loss 1.24 (1.07)	Prec@1 65.33 (61.02)
Val-(28): [500/1000]	Time 0.05 (0.09)	Loss 0.84 (1.07)	Prec@1 68.00 (60.71)
Val-(28): [600/1000]	Time 0.36 (0.09)	Loss 0.91 (1.07)	Prec@1 66.67 (60.80)
Val-(28): [700/1000]	Time 0.05 (0.09)	Loss 0.93 (1.06)	Prec@1 62.67 (60.78)
Val-(28): [800/1000]	Time 0.05 (0.09)	Loss 0.94 (1.07)	Prec@1 64.00 (60.58)
Val-(28): [900/1000]	Time 0.32 (0.09)	Loss 0.76 (1.06)	Prec@1 70.67 (60.74)
 * Prec@1 60.87 Best_prec1 65.43
Test accuracy: 60.868000 h: 0.637646 

==================== The 1-th round ====================
Val-(28): [100/1000]	Time 0.21 (0.10)	Loss 0.76 (1.04)	Prec@1 70.67 (61.50)
Val-(28): [200/1000]	Time 0.05 (0.10)	Loss 1.00 (1.02)	Prec@1 60.00 (61.67)
Val-(28): [300/1000]	Time 0.05 (0.09)	Loss 0.99 (1.03)	Prec@1 69.33 (61.46)
Val-(28): [400/1000]	Time 0.10 (0.09)	Loss 1.13 (1.03)	Prec@1 54.67 (61.54)
Val-(28): [500/1000]	Time 0.18 (0.09)	Loss 1.08 (1.03)	Prec@1 60.00 (61.27)
Val-(28): [600/1000]	Time 0.05 (0.09)	Loss 1.33 (1.04)	Prec@1 48.00 (61.21)
Val-(28): [700/1000]	Time 0.05 (0.09)	Loss 0.81 (1.04)	Prec@1 66.67 (61.05)
Val-(28): [800/1000]	Time 0.05 (0.09)	Loss 0.81 (1.04)	Prec@1 68.00 (61.04)
Val-(28): [900/1000]	Time 0.05 (0.09)	Loss 1.61 (1.04)	Prec@1 40.00 (61.05)
 * Prec@1 61.27 Best_prec1 65.43
Test accuracy: 61.273331 h: 0.623978 

==================== The 2-th round ====================
Val-(28): [100/1000]	Time 0.05 (0.10)	Loss 1.26 (1.07)	Prec@1 60.00 (60.48)
Val-(28): [200/1000]	Time 0.05 (0.10)	Loss 0.73 (1.05)	Prec@1 70.67 (61.12)
Val-(28): [300/1000]	Time 0.05 (0.09)	Loss 1.11 (1.04)	Prec@1 56.00 (61.37)
Val-(28): [400/1000]	Time 0.05 (0.09)	Loss 1.07 (1.04)	Prec@1 64.00 (61.43)
Val-(28): [500/1000]	Time 0.05 (0.09)	Loss 0.80 (1.04)	Prec@1 62.67 (61.51)
Val-(28): [600/1000]	Time 0.05 (0.09)	Loss 1.22 (1.04)	Prec@1 58.67 (61.49)
Val-(28): [700/1000]	Time 0.05 (0.09)	Loss 0.96 (1.04)	Prec@1 66.67 (61.38)
Val-(28): [800/1000]	Time 0.05 (0.09)	Loss 0.76 (1.04)	Prec@1 70.67 (61.50)
Val-(28): [900/1000]	Time 0.05 (0.09)	Loss 1.78 (1.04)	Prec@1 40.00 (61.45)
 * Prec@1 61.63 Best_prec1 65.43
Test accuracy: 61.627998 h: 0.610496 

==================== The 3-th round ====================
Val-(28): [100/1000]	Time 0.08 (0.09)	Loss 1.08 (1.06)	Prec@1 50.67 (60.48)
Val-(28): [200/1000]	Time 0.19 (0.09)	Loss 0.98 (1.05)	Prec@1 64.00 (61.27)
Val-(28): [300/1000]	Time 0.05 (0.09)	Loss 1.23 (1.04)	Prec@1 64.00 (61.29)
Val-(28): [400/1000]	Time 0.05 (0.09)	Loss 0.85 (1.04)	Prec@1 72.00 (61.25)
Val-(28): [500/1000]	Time 0.05 (0.09)	Loss 1.09 (1.05)	Prec@1 53.33 (61.10)
Val-(28): [600/1000]	Time 0.05 (0.09)	Loss 1.08 (1.05)	Prec@1 68.00 (61.05)
Val-(28): [700/1000]	Time 0.05 (0.09)	Loss 1.34 (1.05)	Prec@1 49.33 (61.04)
Val-(28): [800/1000]	Time 0.13 (0.09)	Loss 1.39 (1.05)	Prec@1 42.67 (61.26)
Val-(28): [900/1000]	Time 0.05 (0.09)	Loss 1.28 (1.05)	Prec@1 50.67 (61.21)
 * Prec@1 61.30 Best_prec1 65.43
Test accuracy: 61.296005 h: 0.624320 

==================== The 4-th round ====================
Val-(28): [100/1000]	Time 0.06 (0.10)	Loss 0.50 (1.04)	Prec@1 77.33 (61.06)
Val-(28): [200/1000]	Time 0.08 (0.09)	Loss 1.02 (1.02)	Prec@1 60.00 (61.52)
Val-(28): [300/1000]	Time 0.05 (0.09)	Loss 1.72 (1.05)	Prec@1 44.00 (60.99)
Val-(28): [400/1000]	Time 0.05 (0.09)	Loss 0.86 (1.05)	Prec@1 68.00 (60.94)
Val-(28): [500/1000]	Time 0.05 (0.09)	Loss 0.62 (1.05)	Prec@1 78.67 (60.92)
Val-(28): [600/1000]	Time 0.05 (0.09)	Loss 0.95 (1.05)	Prec@1 65.33 (60.82)
Val-(28): [700/1000]	Time 0.15 (0.09)	Loss 0.89 (1.04)	Prec@1 58.67 (61.02)
Val-(28): [800/1000]	Time 0.05 (0.09)	Loss 0.51 (1.05)	Prec@1 80.00 (60.88)
Val-(28): [900/1000]	Time 0.05 (0.09)	Loss 1.04 (1.05)	Prec@1 61.33 (61.10)
 * Prec@1 61.09 Best_prec1 65.43
Test accuracy: 61.090668 h: 0.634639 

Mean_accuracy: 61.231200 h: 0.626216
===================================== Test is END =====================================

